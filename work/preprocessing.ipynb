{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coursework 1 : Movie Recommendation from Text\n",
    "## Data Preprocessing\n",
    "\n",
    "This notebook has already been processed, and you should be able to skip this part."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports and utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import math\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import spacy\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "import wikipedia # not used here, but can be useful\n",
    "\n",
    "\n",
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid_token(tk):\n",
    "    is_valid = tk.is_alpha\n",
    "    return is_valid and not tk.is_stop\n",
    "\n",
    "def get_lemma(tk):\n",
    "    if tk.pos_ == 'PRON' or tk.lemma_ == '-PRON-':\n",
    "        return tk.text.lower()\n",
    "    return tk.lemma_.lower()\n",
    "\n",
    "def read_wikipedia_page(page_name):\n",
    "    page = wikipedia.page(page_name)\n",
    "    content = page.content\n",
    "    return content\n",
    "\n",
    "# This function is only for wikipedia pages\n",
    "def tokenize_page(page_name):\n",
    "  text = read_wikipedia_page(page_name)\n",
    "  return tokenize_text(text)\n",
    "\n",
    "def tokenize_text(text):\n",
    "  return [\n",
    "    get_lemma(t)\n",
    "    for t in nlp(text)\n",
    "    if valid_token(t)\n",
    "  ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved 42204 \n"
     ]
    }
   ],
   "source": [
    "movies_meta=pd.read_csv(\"data/movie.metadata.tsv\", \n",
    "                         sep='\\t', header=None, usecols=[0,1,2,3,8], \n",
    "                         names=['wID', 'fID', 'title', 'data', 'genres' ])\n",
    "\n",
    "movies_plot=pd.read_csv(\"data/plot_summaries.txt\", \n",
    "                         sep='\\t', header=None, usecols=[0,1], \n",
    "                         names=['wID', 'plot'])\n",
    "\n",
    "movies_merged = pd.merge(movies_meta, movies_plot, on='wID', how='inner')\n",
    "    \n",
    "print(\"Retrieved {} \".format(len(movies_merged)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize Movie plots and build global vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|███▉      | 16843/42204 [25:54<57:18,  7.38it/s]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      " 65%|██████▌   | 27441/42204 [43:08<21:56, 11.21it/s]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      " 89%|████████▉ | 37681/42204 [59:52<06:50, 11.01it/s]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "100%|██████████| 42204/42204 [1:07:33<00:00, 10.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running time: 4053.539579153061 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "vocabulary = set()\n",
    "idf_counter = Counter()\n",
    "\n",
    "countp = 0\n",
    "start_time = time.time()\n",
    "\n",
    "for movie in tqdm(movies_merged.itertuples(), total=len(movies_merged)):\n",
    "#for movie in movies_merged.itertuples():\n",
    "  #print(\"   Processing page {}...\".format(page))\n",
    "  \n",
    "  # Tokens as Set so we count them only once per document\n",
    "  page_words = set(tokenize_text(movie.plot))\n",
    "  vocabulary = vocabulary | page_words\n",
    "  idf_counter.update(page_words)\n",
    "    \n",
    "  # To limit computation  \n",
    "#  countp+=1\n",
    "#   if countp %1000 == 0:\n",
    "#         print(\".\",  end = '')\n",
    "#   if countp > 100:\n",
    "#         break\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "print(\"Running time: {} \".format(elapsed_time))\n",
    "\n",
    "# Save to file\n",
    "with open('data/vocabulary.pk',mode='wb') as vocab_file:\n",
    "    pickle.dump(vocabulary, vocab_file)\n",
    "\n",
    "# with open('data/idf.pk',mode='wb') as idf_file:\n",
    "#     pickle.dump(idf_counter, idf_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute Inverse Document Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 123349\n"
     ]
    }
   ],
   "source": [
    "idf = {\n",
    "  word: math.log(len(movies_merged)/df, 2)  for word, df in idf_counter.items()\n",
    "}\n",
    "\n",
    "print(\"Vocabulary size: {}\".format(len(vocabulary)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precompute TF-IDF of each movie in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 42204/42204 [1:02:07<00:00, 11.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running time: 3727.6427714824677 \n"
     ]
    }
   ],
   "source": [
    "def tf_idf(target_row):\n",
    "  target_words = tokenize_text(target_row['plot'])\n",
    "  tfidf =  {\n",
    "    word: (1 + math.log(_tf, 2)) * idf[word]\n",
    "    for word, _tf in Counter(target_words).items()\n",
    "  }\n",
    "  return tfidf\n",
    "    \n",
    "tfidf_dic = {}\n",
    "#countp=0\n",
    "start_time = time.time()\n",
    "for idx, movie in tqdm(movies_merged.iterrows(), total=len(movies_merged)):\n",
    "    countp+=1\n",
    "    tfidf_dic[movie['wID']] = tf_idf(movie)\n",
    "#     if countp %1000 == 0:\n",
    "#         print(\".\",  end = '')\n",
    "    \n",
    "#     if countp > 100:\n",
    "#          break\n",
    "    \n",
    "elapsed_time = time.time() - start_time\n",
    "print(\"Running time: {} \".format(elapsed_time))\n",
    "\n",
    "with open('data/tf_idf.pk',mode='wb') as tfidf_file:\n",
    "    pickle.dump(tfidf_dic, tfidf_file)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 42204/42204 [00:09<00:00, 4263.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "123349\n",
      "33935\n"
     ]
    }
   ],
   "source": [
    "filtered = {w:f for w, f in idf_counter.items() if f > 3 }\n",
    "\n",
    "idf_filtered = {}\n",
    "for w,f in filtered.items():\n",
    "    idf_filtered[w] = idf[w]\n",
    "\n",
    "tfidf_dic_filter = {}\n",
    "for idx, movie in tqdm(movies_merged.iterrows(), total=len(movies_merged)):\n",
    "    countp+=1\n",
    "    tfidf_dic_filter[movie['wID']] = { k: val for k,val in tfidf_dic[movie['wID']].items() if k in idf_filtered  }\n",
    "\n",
    "print(len(idf_counter))\n",
    "print(len(filtered))    \n",
    "with open('data/tf_idf_small.pk',mode='wb') as tfidf_file:\n",
    "    pickle.dump(tfidf_dic_filter, tfidf_file)        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
