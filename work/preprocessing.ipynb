{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coursework 1 : Movie Recommendation from Text\n",
    "## Data Preprocessing\n",
    "\n",
    "This notebook has already been processed, and you should be able to skip this part."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports and utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import math\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import spacy\n",
    "\n",
    "import wikipedia # not used here, but can be useful\n",
    "\n",
    "\n",
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid_token(tk):\n",
    "    is_valid = tk.is_alpha\n",
    "    return is_valid and not tk.is_stop\n",
    "\n",
    "def get_lemma(tk):\n",
    "    if tk.pos_ == 'PRON' or tk.lemma_ == '-PRON-':\n",
    "        return tk.text.lower()\n",
    "    return tk.lemma_.lower()\n",
    "\n",
    "def read_wikipedia_page(page_name):\n",
    "    page = wikipedia.page(page_name)\n",
    "    content = page.content\n",
    "    return content\n",
    "\n",
    "# This function is only for wikipedia pages\n",
    "def tokenize_page(page_name):\n",
    "  text = read_wikipedia_page(page_name)\n",
    "  return tokenize_text(text)\n",
    "\n",
    "def tokenize_text(text):\n",
    "  return [\n",
    "    get_lemma(t)\n",
    "    for t in nlp(text)\n",
    "    if valid_token(t)\n",
    "  ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved 42204 \n"
     ]
    }
   ],
   "source": [
    "movies_meta=pd.read_csv(\"data/movie.metadata.tsv\", \n",
    "                         sep='\\t', header=None, usecols=[0,1,2,3,8], \n",
    "                         names=['wID', 'fID', 'title', 'data', 'genres' ])\n",
    "\n",
    "movies_plot=pd.read_csv(\"data/plot_summaries.txt\", \n",
    "                         sep='\\t', header=None, usecols=[0,1], \n",
    "                         names=['wID', 'plot'])\n",
    "\n",
    "movies_merged = pd.merge(movies_meta, movies_plot, on='wID', how='inner')\n",
    "    \n",
    "print(\"Retrieved {} \".format(len(movies_merged)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize Movie plots and build global vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    }
   ],
   "source": [
    "vocabulary = set()\n",
    "idf_counter = Counter()\n",
    "\n",
    "countp = 0\n",
    "for movie in movies_merged.itertuples():\n",
    "  #print(\"   Processing page {}...\".format(page))\n",
    "  \n",
    "  # Tokens as Set so we count them only once per document\n",
    "  page_words = set(tokenize_text(movie.plot))\n",
    "  vocabulary = vocabulary | page_words\n",
    "  idf_counter.update(page_words)\n",
    "    \n",
    "  # To limit computation  \n",
    "  countp+=1\n",
    "  if countp %1000 == 0:\n",
    "        print(\".\",  end = '')\n",
    "  if countp > 1100:\n",
    "        break\n",
    "        \n",
    "\n",
    "# Save to file\n",
    "with open('data/vocabulary.pk',mode='wb') as vocab_file:\n",
    "    pickle.dump(vocabulary, vocab_file)\n",
    "\n",
    "# with open('data/idf.pk',mode='wb') as idf_file:\n",
    "#     pickle.dump(idf_counter, idf_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute Inverse Document Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 17090\n"
     ]
    }
   ],
   "source": [
    "idf = {\n",
    "  word: math.log(len(movies_merged)/df, 2)  for word, df in idf_counter.items()\n",
    "}\n",
    "\n",
    "print(\"Vocabulary size: {}\".format(len(vocabulary)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precompute TF-IDF of each movie in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_idf(target_row):\n",
    "  target_words = tokenize_text(target_row['plot'])\n",
    "  tfidf =  {\n",
    "    word: (1 + math.log(_tf, 2)) * idf[word]\n",
    "    for word, _tf in Counter(target_words).items()\n",
    "  }\n",
    "  return tfidf\n",
    "    \n",
    "tfidf_dic = {}\n",
    "countp=0\n",
    "for idx, movie in movies_merged.iterrows():\n",
    "    countp+=1\n",
    "    tfidf_dic[movie['wID']] = tf_idf(movie)\n",
    "    if countp > 1100:\n",
    "        break\n",
    "    \n",
    "\n",
    "with open('data/tf_idf.pk',mode='wb') as tfidf_file:\n",
    "    pickle.dump(tfidf_dic, tfidf_file)    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
